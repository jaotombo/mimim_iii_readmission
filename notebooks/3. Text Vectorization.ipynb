{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c724d0",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7e4618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d82c396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca9\\Documents\\MIMIC-III Text Mining\\mimim_iii_readmission\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False\n",
    "\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "if IN_COLAB:  \n",
    "  # Mount the Google Drive at mount\n",
    "  mount='/content/gdrive'\n",
    "  print(\"Colab: mounting Google drive on \", mount)\n",
    "  # connect your colab with the drive\n",
    "  drive.mount(mount)\n",
    "\n",
    " # Switch to the directory on the Google Drive that you want to use\n",
    "  import os\n",
    "  path_to_repo = mount + \"/My Drive/MIMIC-III Text Mining/mimim_iii_readmission\"\n",
    "\n",
    "else:\n",
    "   path_to_repo = os.path.dirname(os.getcwd())\n",
    "\n",
    "  \n",
    "print(path_to_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ba7f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "session_seed = 42 # set seed for our session\n",
    "include_val = False # set to True if we want to also create a validation set\n",
    "test_proportion = 0.2\n",
    "val_proportion = 0.1\n",
    "\n",
    "MAX_FEATURES = 10000 # maximum number of features\n",
    "min_df = 5 # minimum frequency\n",
    "max_df = 0.8 # maximum frequency\n",
    "N_GRAM = (1,2) # n_gram range\n",
    "\n",
    "icu_stays = True # set to TRUE if we want to have only ICU stays\n",
    "lemmatize = True # set to false if we want to do stemming\n",
    "lemma_tag = str(np.where(lemmatize, \"_lemma\",\"\"))\n",
    "heavier_proc = True # if we want a heavier processing\n",
    "if heavier_proc:\n",
    "    heavier_tag = '_heavier'\n",
    "else:\n",
    "        heavier_tag = ''\n",
    "spacy = True\n",
    "if spacy: lemma_tag = str(np.where(lemmatize, \"_lemma_spacy\",\"\"))\n",
    "\n",
    "if include_val == True:\n",
    "    train_proportion = 1 - test_proportion - val_proportion\n",
    "else:\n",
    "    train_proportion = 1 - test_proportion\n",
    "\n",
    "seed_tag = f'_{session_seed}'\n",
    "\n",
    "random.seed(session_seed)\n",
    "\n",
    "med_7 = False # set to True if we want to use our Med7 preprocessing\n",
    "\n",
    "if med_7:\n",
    "    med_tag = \"_med7\"\n",
    "else:\n",
    "    med_tag = ''\n",
    "    \n",
    "    \n",
    "expanded_def = True # set to True if we want to consider future readmissions and avoid using CMS \n",
    "\n",
    "if icu_stays == True:\n",
    "    icu_folder = 'icu_only'\n",
    "    if expanded_def:\n",
    "        icu_folder = 'expanded'\n",
    "else:\n",
    "    icu_folder = 'all_hosp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1eace08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca9\\Documents\\MIMIC-III Text Mining\\mimim_iii_readmission\\data\\expanded\\\n"
     ]
    }
   ],
   "source": [
    "path_to_data = os.path.join(path_to_repo, \"data\", icu_folder,\"\")\n",
    "print(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2baa836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca9\\Documents\\MIMIC-III Text Mining\\mimim_iii_readmission\\data\\expanded\\processed\\\n"
     ]
    }
   ],
   "source": [
    "path_to_processed = os.path.join(path_to_data,\"processed\",\"\")\n",
    "os.makedirs(path_to_processed, exist_ok=True) # we create the directory if it does not exist\n",
    "print(path_to_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9f4d2",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c903a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(os.path.join(path_to_data,f\"df_cleaned{lemma_tag}{med_tag}{heavier_tag}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ea46a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if med_7:\n",
    "    df['clean'] = df.text_def.apply(lambda x: ' '.join(list(x))) # we need to join all elements into a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "965217ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 391 ms\n",
      "Wall time: 425 ms\n"
     ]
    }
   ],
   "source": [
    "# split into train and test\n",
    "%time train, test = train_test_split(df, test_size = test_proportion, random_state = session_seed, stratify = df.target)\n",
    "if include_val == True:\n",
    "    # furtherly split into validation and train\n",
    "    %time train, val = train_test_split(train, test_size = val_proportion, random_state = session_seed, stratify = train.target)\n",
    "else:\n",
    "    val = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16422ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:(8565, 16)\n",
      "Train:(34260, 16)\n"
     ]
    }
   ],
   "source": [
    "print('Test:{}'.format(test.shape))\n",
    "if include_val: print('Val:{}'.format(val.shape))\n",
    "print('Train:{}'.format(train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d988a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_to_dataframe(df, vectorizer_obj):\n",
    "    \"\"\"\n",
    "    Function to return a dataframe from our vectorizer results\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data = df.toarray(), columns = vectorizer_obj.get_feature_names())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba30881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_features(X_train, X_test, method = 'frequency', include_val = False, X_val = ''):\n",
    "    \"\"\"\n",
    "    Function to perform vectorization of our test sets\n",
    "    X_train, X_test, X_val: our dataframes\n",
    "    method: either 'frequency', 'tf_idf', 'onehot' to employ a different BoW technique\n",
    "    include_val: set to True if we also have a validation dataset\n",
    "    \"\"\"\n",
    "    # initialize our vectorizer\n",
    "    if method == 'tf_idf':\n",
    "        vectorizer = TfidfVectorizer(ngram_range=N_GRAM, min_df=min_df, max_df=max_df, max_features=MAX_FEATURES)\n",
    "    elif method == 'frequency':\n",
    "        vectorizer = CountVectorizer(ngram_range=N_GRAM, min_df=min_df, max_df=max_df, max_features=MAX_FEATURES)\n",
    "    elif method == 'onehot':\n",
    "        vectorizer = CountVectorizer(ngram_range=N_GRAM, min_df=min_df, max_df=max_df, max_features=MAX_FEATURES, binary = True)\n",
    "        \n",
    "    X_train = vectorizer.fit_transform(X_train.clean)\n",
    "    X_train = vectorize_to_dataframe(X_train, vectorizer)\n",
    "    X_test = vectorizer.transform(X_test.clean)\n",
    "    X_test = vectorize_to_dataframe(X_test, vectorizer)\n",
    "    if include_val: \n",
    "        X_val = vectorizer.transform(X_val.clean)\n",
    "        X_val = vectorize_to_dataframe(X_val, vectorizer)\n",
    "    return X_train, X_test, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be1c5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframes(train, test, method, include_val = False, val = '', target = False):\n",
    "    \"\"\"\n",
    "    Function to save our dataframes\n",
    "    train: train set to be saved\n",
    "    val: validation set to be saved\n",
    "    method: method through which we have processed the dataframes, needed as save keyword\n",
    "    include_val: True if we want to save also the test set\n",
    "    test: test set to be saved\n",
    "    \"\"\"\n",
    "    if target == True: \n",
    "        target = 'y_'\n",
    "    else: \n",
    "        target = ''\n",
    "    # need to reset the index\n",
    "    train.reset_index(inplace=True, drop = True)\n",
    "    # save our dataset up to now in feather format\n",
    "    train.to_feather('{}{}train_{}{}{}{}{}'.format(path_to_processed, target, method, seed_tag, lemma_tag, med_tag, heavier_tag))\n",
    "    # need to reset the index\n",
    "    test.reset_index(inplace=True, drop = True)\n",
    "    # save our dataset up to now in feather format\n",
    "    test.to_feather('{}{}test_{}{}{}{}{}'.format(path_to_processed, target, method, seed_tag, lemma_tag, med_tag, heavier_tag))\n",
    "    if include_val:\n",
    "        # need to reset the index\n",
    "        val.reset_index(inplace=True, drop = True)\n",
    "        # save our dataset up to now in feather format\n",
    "        val.to_feather('{}{}val_{}{}{}{}{}'.format(path_to_processed, target, method, seed_tag, lemma_tag, med_tag, heavier_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbed2d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency\n",
      "CPU times: total: 1min 11s\n",
      "Wall time: 1min 11s\n",
      "onehot\n",
      "CPU times: total: 1min 29s\n",
      "Wall time: 1min 30s\n",
      "tf_idf\n",
      "CPU times: total: 1min 19s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "# perform vectorization\n",
    "method_list = ['frequency', 'onehot','tf_idf']\n",
    "\n",
    "for method in method_list:\n",
    "    print(method)\n",
    "    # for each method we perform vectorization\n",
    "    %time x_train, x_test, x_val = vectorize_features(train, test, method = method, include_val = include_val, X_val = val)\n",
    "    # and save the dataframes\n",
    "    save_dataframes(x_train, x_test, method = method, include_val = include_val, val = x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9160a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we also save our target variables\n",
    "if include_val:\n",
    "    save_dataframes(pd.DataFrame(train.target), pd.DataFrame(test.target), method = '', include_val = include_val, val = pd.DataFrame(val.target), target = True)\n",
    "else:\n",
    "    save_dataframes(pd.DataFrame(train.target), pd.DataFrame(test.target), method = '', include_val = include_val, val = '', target = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f67be",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "038896d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(method, include_val = True, target = False):\n",
    "    \"\"\"\n",
    "    Function to load train, test and validation set based on the chosen method\n",
    "    method: string for the processing method we want to load\n",
    "    include_diag: if we want to load the dataframes with the diagnosis text, default True\n",
    "    include_test: if we want to load also the test set, default True\n",
    "    target: if we are importing our target variables\n",
    "    \"\"\"\n",
    "    global path_to_processed\n",
    "    if target == True: \n",
    "        target = 'y_'\n",
    "    else: \n",
    "        target = ''\n",
    "    # load it back\n",
    "    train = pd.read_feather(f'{path_to_processed}{target}train_{method}{seed_tag}{lemma_tag}{med_tag}{heavier_tag}')\n",
    "    test = pd.read_feather(f'{path_to_processed}{target}test_{method}{seed_tag}{lemma_tag}{med_tag}{heavier_tag}')\n",
    "    if include_val == True:\n",
    "        val = pd.read_feather(f'{path_to_processed}{target}val_{method}{seed_tag}{lemma_tag}{med_tag}{heavier_tag}')\n",
    "    else: val = []\n",
    "    return train, test, val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe5d8d",
   "metadata": {},
   "source": [
    "Before performing any technique of dimensionality reduction, we re-load our dataset. In particular we will be applying dimensionality reduction to our standard BoW dataframe (*i.e. with frequency encoding*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3613080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = load_datasets('frequency', include_val = include_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293dc0e",
   "metadata": {},
   "source": [
    "We firstly perform truncated SVD - ie. LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "698009dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = 300, random_state = session_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a9e6a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 6s\n",
      "Wall time: 1min 44s\n",
      "CPU times: total: 4.06 s\n",
      "Wall time: 1.88 s\n"
     ]
    }
   ],
   "source": [
    "%time train_svd = svd.fit_transform(train)\n",
    "train_svd = pd.DataFrame(train_svd)\n",
    "train_svd.columns=[\"F\"+str(i) for i in range(0, len(train_svd.columns))] # we need to have column names otherwise feather can't be used\n",
    "%time test_svd = svd.transform(test)\n",
    "test_svd = pd.DataFrame(test_svd)\n",
    "test_svd.columns=[\"F\"+str(i) for i in range(0, len(test_svd.columns))]\n",
    "if include_val:\n",
    "    %time val_svd = svd.transform(val)\n",
    "    val_svd = pd.DataFrame(val_svd)\n",
    "    val_svd.columns=[\"F\"+str(i) for i in range(0, len(val_svd.columns))]\n",
    "else:\n",
    "    val_svd = ''\n",
    "save_dataframes(train_svd, test_svd, method = 'svd', include_val = include_val, val = val_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc51764",
   "metadata": {},
   "source": [
    "We then perform LDA for topic allocation, with an equivalent number of topics from LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc8afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "907af2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 300, random_state = session_seed, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0df5a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 59s\n",
      "Wall time: 33min 32s\n",
      "CPU times: total: 3.42 s\n",
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%time train_lda = lda.fit_transform(train)\n",
    "train_lda = pd.DataFrame(train_lda)\n",
    "train_lda.columns=[\"F\"+str(i) for i in range(0, len(train_lda.columns))]\n",
    "%time test_lda = lda.transform(test)\n",
    "test_lda = pd.DataFrame(test_lda)\n",
    "test_lda.columns=[\"F\"+str(i) for i in range(0, len(test_lda.columns))]\n",
    "if include_val:\n",
    "    %time val_lda = lda.transform(val)\n",
    "    val_lda = pd.DataFrame(val_lda)\n",
    "    val_lda.columns=[\"F\"+str(i) for i in range(0, len(val_lda.columns))]\n",
    "else:\n",
    "    val_lda = ''\n",
    "save_dataframes(train_lda, test_lda, method = 'lda', include_val = include_val, val = val_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93d37342",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path_to_data}lda{seed_tag}{lemma_tag}{med_tag}{heavier_tag}', 'wb') as file: # and save the fitted model\n",
    "    dill.dump(lda, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
