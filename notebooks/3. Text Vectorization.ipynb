{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c724d0",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e4618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d82c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False\n",
    "\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "if IN_COLAB:  \n",
    "  # Mount the Google Drive at mount\n",
    "  mount='/content/gdrive'\n",
    "  print(\"Colab: mounting Google drive on \", mount)\n",
    "  # connect your colab with the drive\n",
    "  drive.mount(mount)\n",
    "\n",
    " # Switch to the directory on the Google Drive that you want to use\n",
    "  import os\n",
    "  path_to_repo = mount + \"/My Drive/MIMIC-III Text Mining/mimim_iii_readmission\"\n",
    "\n",
    "else:\n",
    "   path_to_repo = os.path.dirname(os.getcwd())\n",
    "\n",
    "  \n",
    "print(path_to_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba7f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "session_seed = 42 # set seed for our session\n",
    "include_val = False # set to True if we want to also create a validation set\n",
    "test_proportion = 0.2\n",
    "val_proportion = 0.1\n",
    "\n",
    "MAX_FEATURES = 10000 # maximum number of features\n",
    "min_df = 5 # minimum frequency\n",
    "max_df = 0.8 # maximum frequency\n",
    "N_GRAM = (1,2) # n_gram range\n",
    "\n",
    "icu_stays = True # set to TRUE if we want to have only ICU stays\n",
    "lemmatize = True # set to false if we want to do stemming\n",
    "lemma_tag = str(np.where(lemmatize, \"_lemma\",\"\"))\n",
    "heavier_proc = True # if we want a heavier processing\n",
    "if heavier_proc:\n",
    "    heavier_tag = '_heavier'\n",
    "else:\n",
    "        heavier_tag = ''\n",
    "spacy = True\n",
    "if spacy: lemma_tag = str(np.where(lemmatize, \"_lemma_spacy\",\"\"))\n",
    "\n",
    "if include_val == True:\n",
    "    train_proportion = 1 - test_proportion - val_proportion\n",
    "else:\n",
    "    train_proportion = 1 - test_proportion\n",
    "\n",
    "seed_tag = f'_{session_seed}'\n",
    "\n",
    "random.seed(session_seed)\n",
    "\n",
    "med_7 = False # set to True if we want to use our Med7 preprocessing\n",
    "\n",
    "if med_7:\n",
    "    med_tag = \"_med7\"\n",
    "else:\n",
    "    med_tag = ''\n",
    "    \n",
    "    \n",
    "expanded_def = True # set to True if we want to consider future readmissions and avoid using CMS \n",
    "\n",
    "if icu_stays == True:\n",
    "    icu_folder = 'icu_only'\n",
    "    if expanded_def:\n",
    "        icu_folder = 'expanded'\n",
    "else:\n",
    "    icu_folder = 'all_hosp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eace08",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = os.path.join(path_to_repo, \"data\", icu_folder,\"\")\n",
    "print(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_processed = os.path.join(path_to_data,\"processed\",\"\")\n",
    "os.makedirs(path_to_processed, exist_ok=True) # we create the directory if it does not exist\n",
    "print(path_to_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9f4d2",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(os.path.join(path_to_data,f\"df_cleaned{lemma_tag}{med_tag}{heavier_tag}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea46a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if med_7:\n",
    "    df['clean'] = df.text_def.apply(lambda x: ' '.join(list(x))) # we need to join all elements into a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965217ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "%time train, test = train_test_split(df, test_size = test_proportion, random_state = session_seed, stratify = df.target)\n",
    "if include_val == True:\n",
    "    # furtherly split into validation and train\n",
    "    %time train, val = train_test_split(train, test_size = val_proportion, random_state = session_seed, stratify = train.target)\n",
    "else:\n",
    "    val = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16422ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test:{}'.format(test.shape))\n",
    "if include_val: print('Val:{}'.format(val.shape))\n",
    "print('Train:{}'.format(train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_to_dataframe(df, vectorizer_obj):\n",
    "    \"\"\"\n",
    "    Function to return a dataframe from our vectorizer results\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data = df.toarray(), columns = vectorizer_obj.get_feature_names())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba30881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_features(X_train, X_test, method = 'frequency', include_val = False, X_val = ''):\n",
    "    \"\"\"\n",
    "    Function to perform vectorization of our test sets\n",
    "    X_train, X_test, X_val: our dataframes\n",
    "    method: either 'frequency', 'tf_idf', 'onehot' to employ a different BoW technique\n",
    "    include_val: set to True if we also have a validation dataset\n",
    "    \"\"\"\n",
    "    # initialize our vectorizer\n",
    "    if method == 'tf_idf':\n",
    "        vectorizer = TfidfVectorizer(ngram_range=N_GRAM, min_df=min_df, max_df=max_df, max_features=MAX_FEATURES)\n",
    "    elif method == 'frequency':\n",
    "        vectorizer = CountVectorizer(ngram_range=N_GRAM, min_df=min_df, max_df=max_df, max_features=MAX_FEATURES)\n",
    "    elif method == 'onehot':\n",
    "        vectorizer = CountVectorizer(ngram_range=N_GRAM, min_df=min_df, max_df=max_df, max_features=MAX_FEATURES, binary = True)\n",
    "        \n",
    "    X_train = vectorizer.fit_transform(X_train.clean)\n",
    "    X_train = vectorize_to_dataframe(X_train, vectorizer)\n",
    "    X_test = vectorizer.transform(X_test.clean)\n",
    "    X_test = vectorize_to_dataframe(X_test, vectorizer)\n",
    "    if include_val: \n",
    "        X_val = vectorizer.transform(X_val.clean)\n",
    "        X_val = vectorize_to_dataframe(X_val, vectorizer)\n",
    "    return X_train, X_test, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframes(train, test, method, include_val = False, val = '', target = False):\n",
    "    \"\"\"\n",
    "    Function to save our dataframes\n",
    "    train: train set to be saved\n",
    "    val: validation set to be saved\n",
    "    method: method through which we have processed the dataframes, needed as save keyword\n",
    "    include_val: True if we want to save also the test set\n",
    "    test: test set to be saved\n",
    "    \"\"\"\n",
    "    if target == True: \n",
    "        target = 'y_'\n",
    "    else: \n",
    "        target = ''\n",
    "    # need to reset the index\n",
    "    train.reset_index(inplace=True, drop = True)\n",
    "    # save our dataset up to now in feather format\n",
    "    train.to_feather('{}{}train_{}{}{}{}{}'.format(path_to_processed, target, method, seed_tag, lemma_tag, med_tag, heavier_tag))\n",
    "    # need to reset the index\n",
    "    test.reset_index(inplace=True, drop = True)\n",
    "    # save our dataset up to now in feather format\n",
    "    test.to_feather('{}{}test_{}{}{}{}{}'.format(path_to_processed, target, method, seed_tag, lemma_tag, med_tag, heavier_tag))\n",
    "    if include_val:\n",
    "        # need to reset the index\n",
    "        val.reset_index(inplace=True, drop = True)\n",
    "        # save our dataset up to now in feather format\n",
    "        val.to_feather('{}{}val_{}{}{}{}{}'.format(path_to_processed, target, method, seed_tag, lemma_tag, med_tag, heavier_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform vectorization\n",
    "method_list = ['frequency', 'onehot','tf_idf']\n",
    "\n",
    "for method in method_list:\n",
    "    print(method)\n",
    "    # for each method we perform vectorization\n",
    "    %time x_train, x_test, x_val = vectorize_features(train, test, method = method, include_val = include_val, X_val = val)\n",
    "    # and save the dataframes\n",
    "    save_dataframes(x_train, x_test, method = method, include_val = include_val, val = x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9160a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we also save our target variables\n",
    "if include_val:\n",
    "    save_dataframes(pd.DataFrame(train.target), pd.DataFrame(test.target), method = '', include_val = include_val, val = pd.DataFrame(val.target), target = True)\n",
    "else:\n",
    "    save_dataframes(pd.DataFrame(train.target), pd.DataFrame(test.target), method = '', include_val = include_val, val = '', target = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f67be",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038896d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(method, include_val = True, target = False):\n",
    "    \"\"\"\n",
    "    Function to load train, test and validation set based on the chosen method\n",
    "    method: string for the processing method we want to load\n",
    "    include_diag: if we want to load the dataframes with the diagnosis text, default True\n",
    "    include_test: if we want to load also the test set, default True\n",
    "    target: if we are importing our target variables\n",
    "    \"\"\"\n",
    "    global path_to_processed\n",
    "    if target == True: \n",
    "        target = 'y_'\n",
    "    else: \n",
    "        target = ''\n",
    "    # load it back\n",
    "    train = pd.read_feather(f'{path_to_processed}{target}train_{method}{seed_tag}{lemma_tag}{med_tag}{heavier_tag}')\n",
    "    test = pd.read_feather(f'{path_to_processed}{target}test_{method}{seed_tag}{lemma_tag}{med_tag}{heavier_tag}')\n",
    "    if include_val == True:\n",
    "        val = pd.read_feather(f'{path_to_processed}{target}val_{method}{seed_tag}{lemma_tag}{med_tag}{heavier_tag}')\n",
    "    else: val = []\n",
    "    return train, test, val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe5d8d",
   "metadata": {},
   "source": [
    "Before performing any technique of dimensionality reduction, we re-load our dataset. In particular we will be applying dimensionality reduction to our standard BoW dataframe (*i.e. with frequency encoding*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = load_datasets('frequency', include_val = include_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293dc0e",
   "metadata": {},
   "source": [
    "We firstly perform truncated SVD - ie. LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698009dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = 300, random_state = session_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9e6a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time train_svd = svd.fit_transform(train)\n",
    "train_svd = pd.DataFrame(train_svd)\n",
    "train_svd.columns=[\"F\"+str(i) for i in range(0, len(train_svd.columns))] # we need to have column names otherwise feather can't be used\n",
    "%time test_svd = svd.transform(test)\n",
    "test_svd = pd.DataFrame(test_svd)\n",
    "test_svd.columns=[\"F\"+str(i) for i in range(0, len(test_svd.columns))]\n",
    "if include_val:\n",
    "    %time val_svd = svd.transform(val)\n",
    "    val_svd = pd.DataFrame(val_svd)\n",
    "    val_svd.columns=[\"F\"+str(i) for i in range(0, len(val_svd.columns))]\n",
    "else:\n",
    "    val_svd = ''\n",
    "save_dataframes(train_svd, test_svd, method = 'svd', include_val = include_val, val = val_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc51764",
   "metadata": {},
   "source": [
    "We then perform LDA for topic allocation, with an equivalent number of topics from LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907af2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 300, random_state = session_seed, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df5a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_lda = lda.fit_transform(train)\n",
    "train_lda = pd.DataFrame(train_lda)\n",
    "train_lda.columns=[\"F\"+str(i) for i in range(0, len(train_lda.columns))]\n",
    "%time test_lda = lda.transform(test)\n",
    "test_lda = pd.DataFrame(test_lda)\n",
    "test_lda.columns=[\"F\"+str(i) for i in range(0, len(test_lda.columns))]\n",
    "if include_val:\n",
    "    %time val_lda = lda.transform(val)\n",
    "    val_lda = pd.DataFrame(val_lda)\n",
    "    val_lda.columns=[\"F\"+str(i) for i in range(0, len(val_lda.columns))]\n",
    "else:\n",
    "    val_lda = ''\n",
    "save_dataframes(train_lda, test_lda, method = 'lda', include_val = include_val, val = val_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d37342",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path_to_data}lda{seed_tag}{lemma_tag}{med_tag}{heavier_tag}', 'wb') as file: # and save the fitted model\n",
    "    dill.dump(lda, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484f6c1",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ffa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01510e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # path for the model\n",
    "    word2vec_med_path = os.path.join(path_to_repo, \"data\", \"word_embeddings\", \"wikipedia-pubmed-and-PMC-w2v.bin\")\n",
    "    # load the model\n",
    "    w2v_med = KeyedVectors.load_word2vec_format(datapath(word2vec_med_path), binary = True)\n",
    "    print(\"Loaded from repository\")\n",
    "except:\n",
    "    # if the code above gives permission denied error, simply load the model (or download it) from the default directory\n",
    "    print(\"No Embeddings Found for Word2vec - Wikipedia/Pubmed/PMC!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d8e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # path for the model\n",
    "    bio_vec_path = os.path.join(path_to_repo, \"data\", \"word_embeddings\", \"BioWordVec_PubMed_MIMICIII_d200.vec.bin\")\n",
    "    # load the model\n",
    "    bio_w2v = KeyedVectors.load_word2vec_format(datapath(bio_vec_path), binary = True)\n",
    "    print(\"Loaded from repository\")\n",
    "except:\n",
    "    # if the code above gives permission denied error, simply load the model (or download it) from the default directory\n",
    "    print(\"No Embeddings Found for BiowordVec!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embedding_feats(list_of_lists, DIMENSION, w2v_model):\n",
    "    \"\"\"\n",
    "    Function that takes in the input text dataset in form of list of lists (or pandas Series) where each sentence is a\n",
    "    list of words all the sentences are inside a list\n",
    "    list_of_lists: our list of sentences\n",
    "    DIMENSION: the dimension of the word embeddings\n",
    "    w2w_model: our word embedding model\n",
    "    credits - https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-2-word2vec-35c8c3b34ee3\n",
    "    \"\"\"\n",
    "    zeros_vector = np.zeros(DIMENSION)\n",
    "    feats = []\n",
    "    missing = set()\n",
    "    missing_sentences = set()\n",
    "    #Traverse over each sentence\n",
    "    for tokens in tqdm(list_of_lists):\n",
    "        # Initially assign zeroes as the embedding vector for the sentence\n",
    "        feat_for_this = zeros_vector\n",
    "        #Count the number of words in the embedding for this sentence\n",
    "        count_for_this = 0\n",
    "        #Traverse over each word of a sentence\n",
    "        for token in tokens:\n",
    "            #Check if the word is in the embedding vector\n",
    "            if token in w2v_model:\n",
    "                #Add the vector of the word to vector for the sentence\n",
    "                feat_for_this += w2v_model[token]\n",
    "                count_for_this +=1\n",
    "            #Else assign the missing word to missing set just to have a look at it\n",
    "            else:\n",
    "                missing.add(token)\n",
    "        #If no words are found in the embedding for the sentence\n",
    "        if count_for_this == 0:\n",
    "            #Assign all zeroes vector for that sentence\n",
    "            feats.append(feat_for_this)\n",
    "            #Assign the missing sentence to missing_sentences just to have a look at it\n",
    "            missing_sentences.add(' '.join(tokens))\n",
    "        #Else take average of the values of the embedding for each word to get the embedding of the sentence\n",
    "        else:\n",
    "            feats.append(feat_for_this/count_for_this)\n",
    "    print(\"Total missing words: {}\".format(len(missing)))\n",
    "    print(\"Total missing sentences: {}\".format(len(missing_sentences)))\n",
    "    # convert our list of arrays to a DataFrame\n",
    "    feats = pd.DataFrame(feats)\n",
    "    return feats, missing, missing_sentences\n",
    "\n",
    "def embedding_diag(list_of_lists, DIMENSION, w2v_model, include_diag = True, lemmatization = True):\n",
    "    \"\"\"\n",
    "    Function to apply word embeddins to both final text and diagnosis\n",
    "    include_diag: True if we want to apply it also to diagnosis embeddings\n",
    "    lemmatization: True if we want to use the lemmatized text\n",
    "    \"\"\"\n",
    "    print(\"\\nFinal Text:\")\n",
    "    if lemmatization == True:\n",
    "      list_vectorization, missing, missing_sentences = embedding_feats(list_of_lists.clean, DIMENSION, w2v_model)\n",
    "    else:\n",
    "      list_vectorization, missing, missing_sentences = embedding_feats(list_of_lists.text, DIMENSION, w2v_model)\n",
    "    missing = list(missing)\n",
    "    missing_sentences = list(missing_sentences)\n",
    "    if include_diag == True:\n",
    "        print(\"\\nDiagnosis:\")\n",
    "        if lemmatization == True:\n",
    "          df_vectors_diag, missing_diag, missing_sentences_diag = embedding_feats(list_of_lists.diagnosis_def, DIMENSION, w2v_model)\n",
    "        else:\n",
    "          df_vectors_diag, missing_diag, missing_sentences_diag = embedding_feats(list_of_lists.diagnosis_def_nolemma, DIMENSION, w2v_model)\n",
    "        df_final = pd.merge(list_vectorization, df_vectors_diag, left_index = True, right_index = True, suffixes = (\"\",\"_diag\"))\n",
    "        missing.append(list(missing_diag))\n",
    "        missing_sentences.append(list(missing_sentences_diag))\n",
    "    else:\n",
    "        df_final = list_vectorization.copy()\n",
    "    return df_final, missing, missing_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd641101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform vectorization WITHOUT LEMMAS\n",
    "embedding_dict = {'W2V_Med': w2v_med, 'Bio_W2V': bio_w2v}\n",
    "include_diag = False\n",
    "\n",
    "for method, embedding in embedding_dict.items():\n",
    "  if method == 'W2V_Med' or method == 'Bio_W2V':\n",
    "    vector_dim = 200\n",
    "  else:\n",
    "    vector_dim = 300\n",
    "  print(method)\n",
    "  print(\"Train Set:\")\n",
    "  %time x_train, missing_train, missing_sentences_train = embedding_diag(train, vector_dim, embedding, include_diag = include_diag)\n",
    "  # make string column names\n",
    "  x_train.columns = [f\"method_{col}\" for col in x_train.columns]\n",
    "  print(\"Validation Set:\")\n",
    "  %time x_test, missing_test, missing_sentences_test = embedding_diag(test, vector_dim, embedding, include_diag = include_diag)\n",
    "  x_test.columns = [f\"method_{col}\" for col in x_test.columns]\n",
    "  if include_val == True:\n",
    "    print(\"Test Set:\")\n",
    "    %time x_val, missing_val, missing_sentences_val = embedding_diag(val, vector_dim, embedding, include_diag = include_diag)\n",
    "    x_val.columns = [f\"method_{col}\" for col in x_val.columns]\n",
    "  else:\n",
    "    x_val = []\n",
    "  save_dataframes(x_train, x_test, method = method, include_val = include_val, val = x_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
