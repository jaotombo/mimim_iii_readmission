{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39c464e0",
   "metadata": {
    "id": "39c464e0"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadefae3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dadefae3",
    "outputId": "e9d79146-ee91-4e19-f7c9-27b4d2a9727d"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import dill\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "try:\n",
    "    from hmeasure import h_score\n",
    "except:\n",
    "    !pip install hmeasure\n",
    "    from hmeasure import h_score\n",
    "try:\n",
    "  from catboost import CatBoostClassifier\n",
    "except:\n",
    "  !pip install catboost\n",
    "  from catboost import CatBoostClassifier\n",
    "\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205b1e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2205b1e7",
    "outputId": "e4543a9c-c463-4fc2-b393-e42fd55b2a1a"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False\n",
    "\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "if IN_COLAB:  \n",
    "  # Mount the Google Drive at mount\n",
    "  mount='/content/gdrive'\n",
    "  print(\"Colab: mounting Google drive on \", mount)\n",
    "  # connect your colab with the drive\n",
    "  drive.mount(mount)\n",
    "\n",
    " # Switch to the directory on the Google Drive that you want to use\n",
    "  import os\n",
    "  path_to_repo = mount + \"/My Drive/MIMIC-III Text Mining/mimim_iii_readmission\"\n",
    "\n",
    "else:\n",
    "   path_to_repo = os.path.dirname(os.getcwd())\n",
    "\n",
    "  \n",
    "print(path_to_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eba6db",
   "metadata": {
    "id": "52eba6db"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "session_seed = 42 # set seed for our session\n",
    "include_val = False # set to True if we want to also create a validation set\n",
    "tune_models = True # set to True if we want to perform parameter tuning\n",
    "\n",
    "icu_stays = True # set to TRUE if we want to have only ICU stays\n",
    "lemmatize = True # set to false if we want to do stemming\n",
    "lemma_tag = str(np.where(lemmatize, \"_lemma\",\"\"))\n",
    "heavier_proc = True # if we want a heavier processing\n",
    "if heavier_proc:\n",
    "    heavier_tag = '_heavier'\n",
    "else:\n",
    "    heavier_tag = ''\n",
    "    \n",
    "spacy = True\n",
    "if spacy: lemma_tag = str(np.where(lemmatize, \"_lemma_spacy\",\"\"))\n",
    "\n",
    "seed_tag = f'_{session_seed}'\n",
    "\n",
    "halving = True # if we want to perform halving tune\n",
    "if tune_models:\n",
    "    if halving:\n",
    "        tune_tag = '_tuned_halv'\n",
    "    else:\n",
    "        tune_tag = '_tuned'   \n",
    "else:\n",
    "    tune_tag = ''\n",
    "\n",
    "random.seed(session_seed)\n",
    "\n",
    "med_7 = False # set to True if we want to use our Med7 preprocessing\n",
    "\n",
    "if med_7:\n",
    "    med_tag = \"_med7\"\n",
    "else:\n",
    "    med_tag = ''\n",
    "    \n",
    "feat_select = False # select True if we want to use Lasso as a feature selection method\n",
    "\n",
    "if feat_select:\n",
    "    feat_tag = \"_featselect\"\n",
    "else:\n",
    "    feat_tag = ''\n",
    "    \n",
    "expanded_def = True # set to True if we want to consider future readmissions and avoid using CMS \n",
    "\n",
    "if icu_stays == True:\n",
    "    icu_folder = 'icu_only'\n",
    "    if expanded_def:\n",
    "        icu_folder = 'expanded'\n",
    "else:\n",
    "    icu_folder = 'all_hosp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f43fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "729f43fa",
    "outputId": "40533f0b-c81c-469c-8954-24ac30711aea"
   },
   "outputs": [],
   "source": [
    "path_to_data = os.path.join(path_to_repo, \"data\", icu_folder,\"\")\n",
    "print(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b264d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a17b264d",
    "outputId": "12557a08-55b2-419f-e6f1-af8347719c31"
   },
   "outputs": [],
   "source": [
    "path_to_processed = os.path.join(path_to_data,\"processed\",\"\")\n",
    "os.makedirs(path_to_processed, exist_ok=True) # we create the directory if it does not exist\n",
    "print(path_to_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03c45a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d03c45a",
    "outputId": "566c6f43-ea46-4432-cc97-17031353915e"
   },
   "outputs": [],
   "source": [
    "path_to_models = os.path.join(path_to_data,\"models\",\"\")\n",
    "os.makedirs(path_to_models, exist_ok=True) # we create the directory if it does not exist\n",
    "print(path_to_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb49c6",
   "metadata": {
    "id": "6cbb49c6"
   },
   "source": [
    "## Train the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677bd47",
   "metadata": {
    "id": "5677bd47"
   },
   "outputs": [],
   "source": [
    "def load_datasets(method, include_val = True, target = False):\n",
    "    \"\"\"\n",
    "    Function to load train, test and validation set based on the chosen method\n",
    "    method: string for the processing method we want to load\n",
    "    include_diag: if we want to load the dataframes with the diagnosis text, default True\n",
    "    include_test: if we want to load also the test set, default True\n",
    "    target: if we are importing our target variables\n",
    "    \"\"\"\n",
    "    global path_to_processed\n",
    "    if target == True: \n",
    "        target = 'y_'\n",
    "    else: \n",
    "        target = ''\n",
    "    # load it back\n",
    "    train = pd.read_feather(f'{path_to_processed}{target}train_{method}{seed_tag}{lemma_tag}{med_tag}{heavier_tag}')\n",
    "    test = pd.read_feather(f'{path_to_processed}{target}test_{method}{seed_tag}{lemma_tag}{med_tag}{heavier_tag}')\n",
    "    if include_val == True:\n",
    "        val = pd.read_feather(f'{path_to_processed}{target}val_{method}{seed_tag}{lemma_tag}{med_tag}{heavier_tag}')\n",
    "    else: val = []\n",
    "    return train, test, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c3a0cd",
   "metadata": {
    "id": "94c3a0cd"
   },
   "outputs": [],
   "source": [
    "y_train, y_test, y_val = load_datasets(method = '', include_val = include_val, target = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0a6f7",
   "metadata": {
    "id": "9dd0a6f7"
   },
   "outputs": [],
   "source": [
    "# initialize a dictionary for the results of all the models\n",
    "final_train = {}\n",
    "final_val = {}\n",
    "final_test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35822f",
   "metadata": {
    "id": "be35822f"
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'log_reg': LogisticRegression(solver = \"saga\", penalty = 'elasticnet', random_state = session_seed, n_jobs = -1) # default penalty is l2, we do lasso\n",
    "    , 'dec_tree': DecisionTreeClassifier(random_state = session_seed)\n",
    "    #, 'bag_tree': BaggingClassifier(base_estimator = DecisionTreeClassifier(), n_estimators = 10, random_state = session_seed, n_jobs = -1)\n",
    "    , 'rand_for': RandomForestClassifier(random_state = session_seed, n_jobs = -1)\n",
    "    , 'gboost': GradientBoostingClassifier(random_state = session_seed)\n",
    "    ,'lightgbm': lgb.LGBMClassifier(random_state = session_seed, n_jobs = -1, deterministic = True)\n",
    "    , 'catboost': CatBoostClassifier(random_seed = session_seed)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beuUkt18xXbf",
   "metadata": {
    "id": "beuUkt18xXbf"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS FOR LOGISTIC REGRESSION -------\n",
    "param_en = {'C': np.logspace(-3, 4, 10), 'l1_ratio':np.linspace(0,1,11) }\n",
    "\n",
    "# PARAMETERS FOR DECISION TREE -------------\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "param_dec = {'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "# PARAMETERS FOR RANDOM FOREST -------\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 5)]\n",
    "\n",
    "# Maximum number of samples per tree\n",
    "max_sampl = list(np.arange(0.01,1,0.2))\n",
    "max_sampl.append(None)\n",
    "# Create the random grid\n",
    "param_rf = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'max_samples': max_sampl}\n",
    "\n",
    "# PARAMETERS FOR GRADIENT BOOSTING --------\n",
    "\n",
    "learn_rate = list(np.linspace(0.01, 1, num = 10))\n",
    "\n",
    "param_gb = {'n_estimators': n_estimators,\n",
    "            'learning_rate': learn_rate,\n",
    "               'max_features': list(np.linspace(0.1, 1, num = 10)),\n",
    "               'max_depth': [int(x) for x in np.linspace(2, 16, num = 11)],\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'subsample': list(np.linspace(0.1, 1, num = 10))}\n",
    "\n",
    "# PARAMETERS FOR LIGHTGBM -----------\n",
    "param_lgb = {'max_depth': max_depth,\n",
    "             'min_data_in_leaf': min_samples_leaf,\n",
    "             'num_iterations': n_estimators,\n",
    "             'learning_rate': learn_rate,\n",
    "             'colsample_bytree': list(np.linspace(0.1, 1, num = 10)),\n",
    "             'subsample': list(np.linspace(0.1, 1, num = 10)),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "# PARAMETERS FOR CATBOOSTING ------\n",
    "\n",
    "param_cat = {'n_estimators': n_estimators,\n",
    "            'learning_rate': learn_rate,\n",
    "               'rsm': list(np.linspace(0.1, 1, num = 10)),\n",
    "               'depth': [int(x) for x in np.linspace(2, 16, num = 11)]\n",
    "            , 'l2_leaf_reg': [1, 2, 3, 4, 5, 7, 9, 15, 20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EzVYyT1S1DuA",
   "metadata": {
    "id": "EzVYyT1S1DuA"
   },
   "outputs": [],
   "source": [
    "param_dictionary = {\n",
    "    'log_reg': param_en\n",
    "    , 'dec_tree': param_dec\n",
    "    #, 'bag_tree': BaggingClassifier(base_estimator = DecisionTreeClassifier(), n_estimators = 10, random_state = session_seed, n_jobs = -1)\n",
    "    , 'rand_for': param_rf\n",
    "    , 'gboost': param_gb\n",
    "    , 'lightgbm': param_lgb\n",
    "    , 'catboost': param_cat\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a83e3e",
   "metadata": {
    "id": "f0a83e3e"
   },
   "outputs": [],
   "source": [
    "method_list = ['frequency', 'onehot','tf_idf', 'svd', 'lda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    " 'catboost': CatBoostClassifier(random_seed = session_seed)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53596b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_scores(estimator, df, y):\n",
    "    \"\"\"\n",
    "    Function to get the main scores\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    probas = estimator.predict_proba(df)[:,1]\n",
    "    results['ROC'] = roc_auc_score(y, probas)\n",
    "    precision, recall, _ = metrics.precision_recall_curve(y, probas)\n",
    "    results['PrRc'] = metrics.auc(recall, precision)\n",
    "    results['HScore'] = h_score(y.to_numpy(), estimator.predict(df))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6553d5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a6553d5b",
    "outputId": "fbcf04f2-3b7f-41ce-e3fd-138d6c4fb5e6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model_name, model in model_dict.items(): \n",
    "    print(model_name)\n",
    "    train_res = {}\n",
    "    val_res = {}\n",
    "    test_res = {}\n",
    "    for method in method_list:\n",
    "        print(method)\n",
    "        train, test, val = load_datasets(method, include_val = include_val) # we load the dataset we want to use\n",
    "        start_time = time.monotonic()\n",
    "        if feat_select:\n",
    "            with open(f'{path_to_models}_log_reg_{method}{tune_tag}{seed_tag}{lemma_tag}{med_tag}{heavier_tag}', 'rb') as file:\n",
    "                # import a previously fitted lasso model\n",
    "                lasso = dill.load(file)\n",
    "            lasso_coef = pd.DataFrame(lasso.coef_, columns = train.columns, index=[\"coefficients\"])\n",
    "            lasso_imp = (lasso_coef.T.abs()/lasso_coef.T.abs().max()).sort_values(by=\"coefficients\",ascending=False)*100\n",
    "            lasso_select = lasso_imp[lasso_imp[\"coefficients\"] != 0]\n",
    "            lasso_var_select = list(lasso_select.index)\n",
    "            train = train[lasso_var_select]\n",
    "            test = test[lasso_var_select]\n",
    "            if include_val: val = val[lasso_var_select]\n",
    "        try:\n",
    "            with open(f'{path_to_models}_{model_name}_{method}{tune_tag}{seed_tag}{lemma_tag}{med_tag}{heavier_tag}{feat_tag}', 'rb') as file:\n",
    "                estimator = dill.load(file)\n",
    "            print('Model already trained')\n",
    "        except:\n",
    "            if tune_models:\n",
    "              # if we want to perform parameter tuning we use randomsearchCV\n",
    "                if halving:\n",
    "                    gridsearch = HalvingRandomSearchCV(model, param_dictionary[model_name], n_candidates = 50, max_resources = 2000, cv=5, n_jobs=-1, \n",
    "                          scoring='roc_auc')\n",
    "                else:\n",
    "                    gridsearch = RandomizedSearchCV(model, param_dictionary[model_name], cv=5, n_jobs=-1, \n",
    "                          scoring=['accuracy','roc_auc'], refit = 'roc_auc')\n",
    "                gridsearch.fit(train, y_train) # we fit our model\n",
    "                estimator = gridsearch.best_estimator_\n",
    "            else:\n",
    "                model.fit(train, y_train) # we fit our model\n",
    "                estimator = model\n",
    "            print('Model successfully trained')\n",
    "            with open(f'{path_to_models}_{model_name}_{method}{tune_tag}{seed_tag}{lemma_tag}{med_tag}{heavier_tag}{feat_tag}', 'wb') as file: # and save the fitted model\n",
    "                dill.dump(estimator, file)\n",
    "            print('Model saved')\n",
    "        end_time = time.monotonic()\n",
    "        print(timedelta(seconds=end_time - start_time))\n",
    "        results_train = get_main_scores(estimator, train, y_train.target)\n",
    "        results_test = get_main_scores(estimator, test, y_test.target)\n",
    "        train_res[method] = results_train\n",
    "        test_res[method] = results_test\n",
    "        print('ROC Training Set: {}'.format(results_train['ROC']))\n",
    "        print('ROC Test Set: {}'.format(results_test['ROC']))\n",
    "        if include_val == True:\n",
    "            results_val = get_main_scores(estimator, val, y_val.target)\n",
    "            val_res[method] = results_val\n",
    "            print('ROC Validation Set: {}'.format(results_val['ROC']))\n",
    "    # finally we add the result lists to our dictionary\n",
    "    final_train[model_name] = train_res\n",
    "    final_val[model_name] = val_res\n",
    "    final_test[model_name] = test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a745e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Then we save all our results\n",
    "    with open(f'{path_to_models}train_results{tune_tag}{lemma_tag}{med_tag}{heavier_tag}{feat_tag}.pkl', 'rb') as file:\n",
    "        results_train = pickle.load(file)\n",
    "    with open(f'{path_to_models}test_results{tune_tag}{lemma_tag}{med_tag}{heavier_tag}{feat_tag}.pkl', 'rb') as file:\n",
    "        results_test = pickle.load(file)\n",
    "    if include_val == True:\n",
    "        with open(f'{path_to_models}val_results{tune_tag}{lemma_tag}{med_tag}{heavier_tag}{feat_tag}.pkl', 'rb') as file:\n",
    "            results_val = pickle.load(file)\n",
    "except:\n",
    "    results_train = {}\n",
    "    results_val = {}\n",
    "    results_test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b03694",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train[session_seed] = final_train\n",
    "if include_val == True: results_val[session_seed] = final_val\n",
    "results_test[session_seed] = final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc9f57",
   "metadata": {
    "id": "17dc9f57"
   },
   "outputs": [],
   "source": [
    "# Then we save all our results\n",
    "with open(f'{path_to_models}train_results{tune_tag}{lemma_tag}{med_tag}{heavier_tag}{feat_tag}.pkl', 'wb') as file:\n",
    "    pickle.dump(results_train, file)\n",
    "with open(f'{path_to_models}test_results{tune_tag}{lemma_tag}{med_tag}{heavier_tag}{feat_tag}.pkl', 'wb') as file:\n",
    "    pickle.dump(results_test, file)\n",
    "if include_val == True:\n",
    "    with open(f'{path_to_models}val_results{tune_tag}{lemma_tag}{med_tag}{heavier_tag}{feat_tag}.pkl', 'wb') as file:\n",
    "        pickle.dump(results_val, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36537b18",
   "metadata": {
    "id": "36537b18"
   },
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079b320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_score(dict_res, model, method, score):\n",
    "    \"\"\"\n",
    "    Function to return the average score for a certain combination of model, method and score\n",
    "    \"\"\"\n",
    "    score_avg = 0\n",
    "    for key in dict_res.keys():\n",
    "        score_avg += dict_res[key][model][method][score]\n",
    "    score_avg = score_avg / len(dict_res.keys())\n",
    "    return score_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f00fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "startrow = 0\n",
    "writer = pd.ExcelWriter(f'{path_to_models}output.xlsx')\n",
    "for score in ['ROC', 'PrRc', 'HScore']:\n",
    "    final_train = []\n",
    "    final_test = []\n",
    "    if include_val: final_val = []\n",
    "    for model in model_dict.keys():\n",
    "        train_res = [model]\n",
    "        test_res = [model]\n",
    "        if include_val: val_res = [model]\n",
    "        for method in method_list:\n",
    "            train_res.append(average_score(results_train, model, method, score))\n",
    "            test_res.append(average_score(results_test, model, method, score))\n",
    "            if include_val: val_res.append(average_score(results_val, model, method, score))\n",
    "        final_train.append(train_res)\n",
    "        final_test.append(test_res)\n",
    "        if include_val: final_val.append(val_res)\n",
    "    print(\"-\"*65)\n",
    "    print(f'\\n{score}\\n')\n",
    "    print('\\n Train Results \\n')\n",
    "    print(tabulate(final_train, headers = method_list))\n",
    "    print(\"-\"*65)\n",
    "    print('\\n Test Results \\n')\n",
    "    print(tabulate(final_test, headers = method_list))\n",
    "    if include_val: \n",
    "        print('\\n Val Results \\n')\n",
    "        print(tabulate(final_val, headers = method_list))\n",
    "        print(\"-\"*65)\n",
    "    for name,df in {'train': final_train, 'test': final_test}.items():\n",
    "        # assuming your behavior here replace with desired result. \n",
    "        df = pd.DataFrame(df, columns = ['models'] + method_list)\n",
    "        df.to_excel(writer,sheet_name='Untuned', startrow=startrow +1)\n",
    "        worksheet = writer.sheets['Untuned']\n",
    "        worksheet.write(startrow, 0, f'{score}: {name}')\n",
    "        startrow += (df.shape[0] + 2)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "4. Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
