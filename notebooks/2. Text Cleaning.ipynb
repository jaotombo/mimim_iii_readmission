{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee1f309",
   "metadata": {
    "id": "5ee1f309"
   },
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edf2fda9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1647512873831,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "edf2fda9",
    "outputId": "b6ca999e-4697-4f43-ec4e-de6b59689b8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luca9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luca9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "import spacy\n",
    "import scispacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "##for clustering\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from unidecode import unidecode\n",
    "\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca01193",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1724,
     "status": "ok",
     "timestamp": 1647512726064,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "0ca01193",
    "outputId": "e3d048e0-2a68-4ae3-8bd3-7c08325432a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca9\\Documents\\MIMIC-III Text Mining\\mimim_iii_readmission\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False\n",
    "\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "if IN_COLAB:  \n",
    "  # Mount the Google Drive at mount\n",
    "  mount='/content/gdrive'\n",
    "  print(\"Colab: mounting Google drive on \", mount)\n",
    "  # connect your colab with the drive\n",
    "  drive.mount(mount)\n",
    "\n",
    " # Switch to the directory on the Google Drive that you want to use\n",
    "  import os\n",
    "  path_to_repo = mount + \"/My Drive/MIMIC-III Text Mining/mimim_iii_readmission\"\n",
    "\n",
    "else:\n",
    "   path_to_repo = os.path.dirname(os.getcwd())\n",
    "\n",
    "  \n",
    "print(path_to_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6fda8c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1647512726065,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "e6fda8c7",
    "outputId": "709c36d1-cef0-4b77-ceab-4f3f2bc8dc40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca9\\Documents\\MIMIC-III Text Mining\\mimim_iii_readmission\\data\\\n"
     ]
    }
   ],
   "source": [
    "path_to_data = os.path.join(path_to_repo, \"data\",\"\")\n",
    "print(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da440ed",
   "metadata": {
    "executionInfo": {
     "elapsed": 6396,
     "status": "ok",
     "timestamp": 1647512732458,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "4da440ed"
   },
   "outputs": [],
   "source": [
    "df = pd.read_feather(os.path.join(path_to_data,\"df_cleaned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "146e6f83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1647512732458,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "146e6f83",
    "outputId": "99e735b9-6cdf-4c87-b51d-8613c9946a58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33309 entries, 0 to 33308\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   index            33309 non-null  int64         \n",
      " 1   subject_id       33309 non-null  int64         \n",
      " 2   hadm_id          33309 non-null  int64         \n",
      " 3   admittime        33309 non-null  datetime64[ns]\n",
      " 4   dischtime        33309 non-null  datetime64[ns]\n",
      " 5   first_careunit   33309 non-null  object        \n",
      " 6   last_careunit    33309 non-null  object        \n",
      " 7   age              33309 non-null  float64       \n",
      " 8   gender           33309 non-null  object        \n",
      " 9   marital_status   31747 non-null  object        \n",
      " 10  insurance        33309 non-null  object        \n",
      " 11  diagnosis        33308 non-null  object        \n",
      " 12  text             33309 non-null  object        \n",
      " 13  next_readmit_dt  33309 non-null  float64       \n",
      " 14  target           33309 non-null  int32         \n",
      "dtypes: datetime64[ns](2), float64(2), int32(1), int64(3), object(7)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc9fc10",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1647512732459,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "cbc9fc10"
   },
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    x = \" \".join(x.split())\n",
    "    x= \" \".join((\" \".join(x.split(\"[**\"))).split(\"**]\"))\n",
    "    x = re.sub(r\"\\([^()]*\\)\", \"\", x)\n",
    "    key_value_strip =(x.split(\":\"))\n",
    "    # remove all sub strings which have a length lesser than 50 characters\n",
    "    string = \" \".join([sub_unit for sub_unit in key_value_strip if len(sub_unit)>50])\n",
    "    x = re.sub(r\"(\\d+)+(\\.|\\))\", \"\", string) # remove all serialization eg 1. 1)\n",
    "    x = re.sub(r\"(\\*|\\?|=)+\", \"\", x) # removing all *, ? and =\n",
    "    x = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", x) ## removing consecutive duplicate words\n",
    "    x = x.replace(\"FOLLOW UP\", \"FOLLOWUP\")\n",
    "    x = x.replace(\"FOLLOW-UP\", \"FOLLOWUP\")\n",
    "    x = re.sub(r\"(\\b)(f|F)(irst)(\\b)?[\\d\\-\\d]*(\\s)*(\\b)?(n|N)(ame)[\\d\\-\\d]*(\\s)*[\\d\\-\\d]*(\\b)\",\"\",x)# remove firstname\n",
    "    x = re.sub(r\"(\\b)(l|L)(ast)(\\b)?[\\d\\-\\d]*(\\s)*(\\b)?(n|N)(ame)[\\d\\-\\d]*(\\s)*[\\d\\-\\d]*(\\b)\", \"\", x) # remove lastname\n",
    "    x = re.sub(r\"(\\b)(d|D)\\.?(r|R)\\.?(\\b)\", \"\", x) # remove Dr abreviation\n",
    "    x = re.sub(r\"(\\b)(m|M)\\.?(d|D)\\.?(\\b)\", \"\", x) # remove M.D. abreviation\n",
    "    x = re.sub(r\"([^A-Za-z0-9\\s](\\s)){2,}\", \"\", x)# remove consecutive punctuations\n",
    "\n",
    "    return(x.replace(\"  \", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "100751a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123130,
     "status": "ok",
     "timestamp": 1647512855584,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "100751a5",
    "outputId": "7378df21-56b8-4ea3-f5eb-789bc46db5f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 28s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%time df[\"text\"] = df[\"text\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeff07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') # nltk stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23883ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^a-z\\s]')\n",
    "STOPWORDS = set(stopwords.words('english')) # import stopwords from nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78978d",
   "metadata": {},
   "source": [
    "Expanded stopwords list from: https://github.com/kavgan/clinical-concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ed2d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import a text file with a list of additional stopwords\n",
    "clinical_stopwords = open(os.path.join(path_to_data,\"stopwords.txt\")).read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "164773e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS |= set(clinical_stopwords) # we merge the two sets of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b87900cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True) # we initialize our stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "168a7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text) :\n",
    "    \"\"\"\n",
    "        text: a string        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "        \n",
    "    text = text.lower() # lowercase text\n",
    "    text = unidecode((text))\n",
    "    text = REPLACE_BY_SPACE_RE.sub(\" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub(\" \",text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text =  \" \".join([stemmer.stem(x) for x in text.split()])\n",
    "    text =  \" \".join([x for x in text.split()])\n",
    "    text =  \" \".join([x for x in text.split()if x not in STOPWORDS]) # delete stopwords from text\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2955f20b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6min 8s\n",
      "Wall time: 6min 11s\n"
     ]
    }
   ],
   "source": [
    "%time df['clean'] = df.text.apply(lambda x: text_prepare(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e546cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to reset the index\n",
    "df.reset_index(inplace=True, drop = True)\n",
    "# save our dataset up to now in feather format\n",
    "df.to_feather(f'{path_to_data}df_cleaned')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[COLAB] 2. Text Cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
