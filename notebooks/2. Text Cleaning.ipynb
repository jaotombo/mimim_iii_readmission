{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee1f309",
   "metadata": {
    "id": "5ee1f309"
   },
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf2fda9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1647512873831,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "edf2fda9",
    "outputId": "b6ca999e-4697-4f43-ec4e-de6b59689b8a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "import spacy\n",
    "import scispacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "##for clustering\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "try:\n",
    "  from unidecode import unidecode\n",
    "except:\n",
    "  !pip install unidecode\n",
    "  from unidecode import unidecode\n",
    "\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm', disable = ['parser','ner'])\n",
    "except:\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load('en_core_web_sm', disable = ['parser','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca01193",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1724,
     "status": "ok",
     "timestamp": 1647512726064,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "0ca01193",
    "outputId": "e3d048e0-2a68-4ae3-8bd3-7c08325432a4"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False\n",
    "\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "if IN_COLAB:  \n",
    "  # Mount the Google Drive at mount\n",
    "  mount='/content/gdrive'\n",
    "  print(\"Colab: mounting Google drive on \", mount)\n",
    "  # connect your colab with the drive\n",
    "  drive.mount(mount)\n",
    "\n",
    " # Switch to the directory on the Google Drive that you want to use\n",
    "  import os\n",
    "  path_to_repo = mount + \"/My Drive/MIMIC-III Text Mining/mimim_iii_readmission\"\n",
    "\n",
    "else:\n",
    "   path_to_repo = os.path.dirname(os.getcwd())\n",
    "\n",
    "  \n",
    "print(path_to_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd646af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "icu_stays = True # set to TRUE if we want to have only ICU stays\n",
    "lemmatize = False # set to false if we want to do stemming\n",
    "lemma_tag = str(np.where(lemmatize, \"_lemma\",\"\"))\n",
    "heavier_proc = True # if we want a heavier processing\n",
    "if heavier_proc:\n",
    "    heavier_tag = '_heavier'\n",
    "else:\n",
    "    heavier_tag = ''\n",
    "spacy = False\n",
    "if spacy: lemma_tag = str(np.where(lemmatize, \"_lemma_spacy\",\"\"))\n",
    "    \n",
    "expanded_def = True # set to True if we want to consider future readmissions and avoid using CMS \n",
    "\n",
    "if icu_stays == True:\n",
    "    icu_folder = 'icu_only'\n",
    "    if expanded_def:\n",
    "        icu_folder = 'expanded'\n",
    "else:\n",
    "    icu_folder = 'all_hosp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fda8c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1647512726065,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "e6fda8c7",
    "outputId": "709c36d1-cef0-4b77-ceab-4f3f2bc8dc40"
   },
   "outputs": [],
   "source": [
    "path_to_data = os.path.join(path_to_repo, \"data\", icu_folder,\"\")\n",
    "print(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da440ed",
   "metadata": {
    "executionInfo": {
     "elapsed": 6396,
     "status": "ok",
     "timestamp": 1647512732458,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "4da440ed"
   },
   "outputs": [],
   "source": [
    "df = pd.read_feather(os.path.join(path_to_data,\"df_cleaned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e6f83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1647512732458,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "146e6f83",
    "outputId": "99e735b9-6cdf-4c87-b51d-8613c9946a58"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc9fc10",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1647512732459,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "cbc9fc10"
   },
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    x = \" \".join(x.split())\n",
    "    x= \" \".join((\" \".join(x.split(\"[**\"))).split(\"**]\"))\n",
    "    x = re.sub(r\"\\([^()]*\\)\", \"\", x)\n",
    "    key_value_strip =(x.split(\":\"))\n",
    "    # remove all sub strings which have a length lesser than 50 characters\n",
    "    string = \" \".join([sub_unit for sub_unit in key_value_strip if len(sub_unit)>50])\n",
    "    x = re.sub(r\"(\\d+)+(\\.|\\))\", \"\", string) # remove all serialization eg 1. 1)\n",
    "    x = re.sub(r\"(\\*|\\?|=)+\", \"\", x) # removing all *, ? and =\n",
    "    x = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", x) ## removing consecutive duplicate words\n",
    "    x = x.replace(\"FOLLOW UP\", \"FOLLOWUP\")\n",
    "    x = x.replace(\"FOLLOW-UP\", \"FOLLOWUP\")\n",
    "    x = re.sub(r\"(\\b)(f|F)(irst)(\\b)?[\\d\\-\\d]*(\\s)*(\\b)?(n|N)(ame)[\\d\\-\\d]*(\\s)*[\\d\\-\\d]*(\\b)\",\"\",x)# remove firstname\n",
    "    x = re.sub(r\"(\\b)(l|L)(ast)(\\b)?[\\d\\-\\d]*(\\s)*(\\b)?(n|N)(ame)[\\d\\-\\d]*(\\s)*[\\d\\-\\d]*(\\b)\", \"\", x) # remove lastname\n",
    "    x = re.sub(r\"(\\b)(d|D)\\.?(r|R)\\.?(\\b)\", \"\", x) # remove Dr abreviation\n",
    "    x = re.sub(r\"(\\b)(m|M)\\.?(d|D)\\.?(\\b)\", \"\", x) # remove M.D. abreviation\n",
    "    x = re.sub(r\"([^A-Za-z0-9\\s](\\s)){2,}\", \"\", x)# remove consecutive punctuations\n",
    "\n",
    "    return(x.replace(\"  \", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100751a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123130,
     "status": "ok",
     "timestamp": 1647512855584,
     "user": {
      "displayName": "Luca Adorni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt-jhhB0G329szPu-ul9xR7_wbTqibbe3M-vnKJQ=s64",
      "userId": "07135966571450304185"
     },
     "user_tz": -60
    },
    "id": "100751a5",
    "outputId": "7378df21-56b8-4ea3-f5eb-789bc46db5f5"
   },
   "outputs": [],
   "source": [
    "%time df[\"text\"] = df[\"text\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') # nltk stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23883ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^a-z\\s]')\n",
    "STOPWORDS = set(stopwords.words('english')) # import stopwords from nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78978d",
   "metadata": {},
   "source": [
    "Expanded stopwords list from: https://github.com/kavgan/clinical-concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import a text file with a list of additional stopwords\n",
    "clinical_stopwords = open(os.path.join(path_to_repo, \"data\",\"stopwords.txt\")).read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the source !!!\n",
    "\n",
    "med_stopwords = {'street',\n",
    "'address',\n",
    "'required',\n",
    "'powder',\n",
    "'developed',\n",
    "'intermittently',\n",
    "'weekly',\n",
    "'later',\n",
    "'echo',\n",
    "'commands',\n",
    "'comfort',\n",
    "'back',\n",
    "'ultimately',\n",
    "'complete',\n",
    "'daughter',\n",
    "'nutrition',\n",
    "'range',\n",
    "'knee',\n",
    "'subsequently',\n",
    "'summary',\n",
    "'upon',\n",
    "'service',\n",
    "'completed',\n",
    "'consistent',\n",
    "'pattern',\n",
    "'woman',\n",
    "'address',\n",
    "'inhaled',\n",
    "'times',\n",
    "'count',\n",
    "'number',\n",
    "'underwent',\n",
    "'post',\n",
    "'oral',\n",
    "'general',\n",
    "'recommend',\n",
    "'goal',\n",
    "'remains',\n",
    "'need',\n",
    "'report',\n",
    "'solution',\n",
    "'female',\n",
    "'exploratory',\n",
    "'level',\n",
    "'poor',\n",
    "'aggressive',\n",
    "'support',\n",
    "'determined',\n",
    "'hand',\n",
    "'instructions',\n",
    "'follow',\n",
    "'rehabilitation',\n",
    "'large',\n",
    "'decreased',\n",
    "'stay',\n",
    "'four',\n",
    "'hours',\n",
    "'intake',\n",
    "'name',\n",
    "'patient',\n",
    "'access',\n",
    "'good',\n",
    "'comfortable',\n",
    "'type',\n",
    "'initials',\n",
    "'external',\n",
    "'percent',\n",
    "'descending',\n",
    "'output',\n",
    "'collection',\n",
    "'stop',\n",
    "'presented',\n",
    "'unit',\n",
    "'name',\n",
    "'positive',\n",
    "'number',\n",
    "'dictated',\n",
    "'line',\n",
    "'plus',\n",
    "'date',\n",
    "'active',\n",
    "'done',\n",
    "'records',\n",
    "'state',\n",
    "'month',\n",
    "'notable',\n",
    "'requiring',\n",
    "'factor',\n",
    "'current',\n",
    "'male',\n",
    "'history',\n",
    "'number',\n",
    "'completed',\n",
    "'tenderness',\n",
    "'ward',\n",
    "'name',\n",
    "'office',\n",
    "'port',\n",
    "'impression',\n",
    "'trace',\n",
    "'improvement',\n",
    "'group',\n",
    "'scan',\n",
    "'given',\n",
    "'patient',\n",
    "'laboratory',\n",
    "'right',\n",
    "'upper',\n",
    "'however',\n",
    "'patient',\n",
    "'volume',\n",
    "'limited',\n",
    "'suggestive',\n",
    "'presents',\n",
    "'year',\n",
    "'also',\n",
    "\"mg\",\n",
    "\"ml\",\n",
    "\"mm\",\n",
    "\"unchanged\",\n",
    "\"normal\",\n",
    "\"admissions\",\n",
    "\"social\"\n",
    "}\n",
    "if heavier_proc == False: med_stopwords = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164773e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS |= set(clinical_stopwords) | med_stopwords # we merge the two sets of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87900cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True) # we initialize our stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text) :\n",
    "    \"\"\"\n",
    "        text: a string        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "        \n",
    "    text = text.lower() # lowercase text\n",
    "    text = unidecode((text))\n",
    "    text = REPLACE_BY_SPACE_RE.sub(\" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub(\" \",text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    if lemmatize == True:\n",
    "        if spacy == True:\n",
    "            doc = nlp(text)\n",
    "            text = \" \".join([token.lemma_ for token in doc])\n",
    "        else:\n",
    "            text =  \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
    "    else:\n",
    "        text =  \" \".join([stemmer.stem(x) for x in text.split()])\n",
    "    text =  \" \".join([x for x in text.split()])\n",
    "   \n",
    "    return text\n",
    "\n",
    "STOPWORDS = \" \".join([x for x in STOPWORDS]) # we transform our stopwords list into a text\n",
    "\n",
    "STOPWORDS = text_prepare(STOPWORDS) # then pre process it to get lemmas\n",
    "\n",
    "STOPWORDS = [x for x in STOPWORDS.split()] # finally re-transform it into a list\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text =  \" \".join([x for x in text.split()if x not in STOPWORDS]) # delete stopwords from text\n",
    "    return text\n",
    "\n",
    "def final_text(text):\n",
    "    text = text_prepare(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955f20b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time df['clean'] = df.text.apply(lambda x: final_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e546cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to reset the index\n",
    "df.reset_index(inplace=True, drop = True)\n",
    "# save our dataset up to now in feather format\n",
    "df.to_feather(f'{path_to_data}df_cleaned{lemma_tag}{heavier_tag}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[COLAB] 2. Text Cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
